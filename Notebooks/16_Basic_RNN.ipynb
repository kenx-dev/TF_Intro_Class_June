{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "* Recurrent networks share their weights over time, thus \"recurrent\"\n",
    "* This allows them to have a kind of \"memory\" about what happened earlier in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../pics/RNN_loop_unrolled.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we create an RNN composed of a layer of five recurrent neurons using ReLU for our activation function. \n",
    "* In this simple case, we are assuming that the RNN runs over only two-time steps, taking input vectors of size 3 at each time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "X1 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "X2 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "\n",
    "Wx = tf.get_variable(\"Wx\", shape=[n_inputs,n_neurons], dtype=tf.float32, initializer=None, regularizer=None, trainable=True, collections=None)\n",
    "\n",
    "Wy = tf.get_variable(\"Wy\", shape=[n_neurons,n_neurons], dtype=tf.float32, initializer=None, regularizer=None, trainable=True, collections=None)\n",
    "\n",
    "b = tf.get_variable(\"b\", shape=[1,n_neurons], dtype=tf.float32, initializer=None, regularizer=None, trainable=True, collections=None)\n",
    "\n",
    "Y1 = tf.nn.relu(tf.matmul(X1, Wx) + b)\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1, Wy) + tf.matmul(X2, Wx) + b)\n",
    "\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This network looks much like a two-layer feedforward neural network, but both layers share the same weights and bias vectors. \n",
    "* Also, we feed inputs at each layer and receive outputs from each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch: instance 0,instance 1,instance 2,instance 3\n",
    "X1_batch = np.array([[0, 2, 3], [2, 8, 9], [5, 3, 8], [3, 2, 9]]) # t = 0\n",
    "X2_batch = np.array([[5, 6, 8], [1, 0, 0], [8, 2, 0], [2, 3, 6]]) # t = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* These mini-batches contain four instances, each with an input sequence composed of exactly two inputs. \n",
    "* At the end, Y1_val and Y2_val contain the outputs of the network at both time steps for all neurons and all instances in the mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.4328816   2.6566072   0.27672583  0.          0.        ]\n",
      " [10.837278   10.352432    1.8948631   1.8187337   0.        ]\n",
      " [10.265583    8.670134    2.654381    2.2625995   0.        ]\n",
      " [ 9.558879    8.379587    0.49371815  1.1972008   0.        ]]\n",
      "\n",
      "[[13.087704  11.129164   2.7504628  1.9635859  0.       ]\n",
      " [ 7.688302   3.9405813  0.         0.         0.       ]\n",
      " [10.975104   7.2570753  4.2138577  3.329887   0.7585387]\n",
      " [12.749466  10.189095   0.         0.         0.       ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\tinit_op.run()\n",
    "\tY1_val, Y2_val = sess.run([Y1, Y2], feed_dict={X1: X1_batch, X2: X2_batch})\n",
    "\n",
    "print(Y1_val) # output at t = 0\n",
    "print()\n",
    "print(Y2_val) # output at t = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.6242257   0.54043114  0.98715794 -0.9825246   0.9693041 ]\n",
      "  [-0.9560957   0.9865212   0.99997306 -0.9999989   0.9999999 ]]\n",
      "\n",
      " [[-0.67390215  0.9120273   0.99933183 -0.9999703   1.        ]\n",
      "  [ 0.45531037  0.33025947 -0.95130485  0.41533625 -0.05823964]]\n",
      "\n",
      " [[-0.6646165   0.9239195   0.99911267 -0.9999819   1.        ]\n",
      "  [ 0.47305995  0.8370427  -0.7788391  -0.98684967  0.9999991 ]]\n",
      "\n",
      " [[ 0.9198561  -0.01989019 -0.99921227  0.64939207  0.99997777]\n",
      "  [-0.23007126 -0.20524526  0.9995201  -0.9977988   0.9999862 ]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "n_steps = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "seq_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons)\n",
    "output_seqs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "# note that here instance one is not as long as the other sequences\n",
    "# so we padded it with a zero vector\n",
    "X_batch = np.array([\n",
    "                   [[0, 2, 3], [2, 8, 9]], # instance 0\n",
    "                   [[5, 6, 8], [0, 0, 0]], # instance 1 (padded with a zero vector)\n",
    "                   [[6, 7, 8], [6, 5, 4]], # instance 2\n",
    "                   [[8, 2, 0], [2, 3, 6]], # instance 3\n",
    "                  ])\n",
    "seq_length_batch = np.array([3, 4, 3, 5])\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        init_op.run()\n",
    "        outputs_val, states_val = sess.run([output_seqs, states], feed_dict={X: X_batch, seq_length: seq_length_batch})\n",
    "\n",
    "print(outputs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
